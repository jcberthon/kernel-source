From: David Woodhouse <dwmw@amazon.co.uk>
Date: Fri, 12 Jan 2018 17:49:25 +0000
Subject: xen/x86/retpoline: Fill RSB on context switch for affected CPUs
Patch-mainline: Never, SUSE-Xen specific
References: bsc#1068032 CVE-2017-5754

On context switch from a shallow call stack to a deeper one, as the CPU
does 'ret' up the deeper side it may encounter RSB entries (predictions for
where the 'ret' goes to) which were populated in userspace.

This is problematic if neither SMEP nor KPTI (the latter of which marks
userspace pages as NX for the kernel) are active, as malicious code in
userspace may then be executed speculatively.

Overwrite the CPU's return prediction stack with calls which are predicted
to return to an infinite loop, to "capture" speculation if this
happens. This is required both for retpoline, and also in conjunction with
IBRS for !SMEP && !KPTI.

On Skylake+ the problem is slightly different, and an *underflow* of the
RSB may cause errant branch predictions to occur. So there it's not so much
overwrite, as *filling* the RSB to attempt to prevent it getting
empty. This is only a partial solution for Skylake+ since there are many
other conditions which may result in the RSB becoming empty. The full
solution on Skylake+ is to use IBRS, which will prevent the problem even
when the RSB becomes empty. With IBRS, the RSB-stuffing will not be
required on context switch.

[ tglx: Added missing vendor check and slighty massaged comments and
  	changelog ]

Signed-off-by: David Woodhouse <dwmw@amazon.co.uk>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Acked-by: Arjan van de Ven <arjan@linux.intel.com>
Cc: gnomes@lxorguk.ukuu.org.uk
Cc: Rik van Riel <riel@redhat.com>
Cc: Andi Kleen <ak@linux.intel.com>
Cc: Josh Poimboeuf <jpoimboe@redhat.com>
Cc: thomas.lendacky@amd.com
Cc: Peter Zijlstra <peterz@infradead.org>
Cc: Linus Torvalds <torvalds@linux-foundation.org>
Cc: Jiri Kosina <jikos@kernel.org>
Cc: Andy Lutomirski <luto@amacapital.net>
Cc: Dave Hansen <dave.hansen@intel.com>
Cc: Kees Cook <keescook@google.com>
Cc: Tim Chen <tim.c.chen@linux.intel.com>
Cc: Greg Kroah-Hartman <gregkh@linux-foundation.org>
Cc: Paul Turner <pjt@google.com>
Link: https://lkml.kernel.org/r/1515779365-9032-1-git-send-email-dwmw@amazon.co.uk

Acked-by: Borislav Petkov <bp@suse.de>
Automatically created from "patches.arch/14.1-x86-retpoline-fill-rsb-on-context-switch-for-affected-cpus.patch" by xen-port-patches.py

--- a/arch/x86/include/mach-xen/asm/system.h
+++ b/arch/x86/include/mach-xen/asm/system.h
@@ -6,6 +6,7 @@
 #include <asm/cpufeature.h>
 #include <asm/cmpxchg.h>
 #include <asm/nops.h>
+#include <asm/nospec-branch.h>
 #include <asm/hypervisor.h>
 
 #include <linux/kernel.h>
@@ -63,6 +64,22 @@ extern void show_regs_common(void);
 #endif	/* CC_STACKPROTECTOR */
 
 /*
+ * When switching from a shallower to a deeper call stack
+ * the RSB may either underflow or use entries populated
+ * with userspace addresses. On CPUs where those concerns
+ * exist, overwrite the RSB with entries which capture
+ * speculative execution to prevent attack.
+ */
+#ifdef CONFIG_RETPOLINE
+#define __switch_fill_rsb \
+        ALTERNATIVE("jmp 10f\n\t" ASM_NOP3, ASM_NOP5, X86_FEATURE_RSB_CTXSW)	\
+	__stringify(__FILL_RETURN_BUFFER(%%ebx,RSB_CLEAR_LOOPS,%%esp)) "\n\t"	\
+	"10:\n"
+#else
+#define __switch_fill_rsb
+#endif
+
+/*
  * Saving eflags is important. It switches not only IOPL between tasks,
  * it also protects other tasks from NT leaking through sysenter etc.
  */
@@ -84,6 +101,7 @@ do {									\
 		     "movl $1f,%[prev_ip]\n\t"	/* save    EIP   */	\
 		     "pushl %[next_ip]\n\t"	/* restore EIP   */	\
 		     __switch_canary					\
+		     __switch_fill_rsb "\n\t"				\
 		     "jmp __switch_to\n"	/* regparm call  */	\
 		     "1:\t"						\
 		     "popl %%ebp\n\t"		/* restore EBP   */	\
@@ -147,11 +165,31 @@ do {									\
 #define THREAD_RETURN_SYM
 #endif
 
+/*
+ * When switching from a shallower to a deeper call stack
+ * the RSB may either underflow or use entries populated
+ * with userspace addresses. On CPUs where those concerns
+ * exist, overwrite the RSB with entries which capture
+ * speculative execution to prevent attack.
+ *
+ * bp: use the old jmp labels and fix the padding so that
+ * the original insn is always >= replacement.
+ */
+#ifdef CONFIG_RETPOLINE
+#define __switch_fill_rsb \
+        ALTERNATIVE("jmp 1f\n\t" ASM_NOP3, ASM_NOP5, X86_FEATURE_RSB_CTXSW)	\
+	__stringify(__FILL_RETURN_BUFFER(%%rbx,RSB_CLEAR_LOOPS,%%rsp)) "\n\t"	\
+	"1:\n"
+#else
+#define __switch_fill_rsb
+#endif
+
 /* Save restore flags to clear handle leaking NT */
 #define switch_to(prev, next, last) \
 	asm volatile(SAVE_CONTEXT					  \
 	     "movq %%rsp,%P[threadrsp](%[prev])\n\t" /* save RSP */	  \
 	     "movq %P[threadrsp](%[next]),%%rsp\n\t" /* restore RSP */	  \
+	     __switch_fill_rsb "\n\t"					  \
 	     "call __switch_to\n\t"					  \
 	     THREAD_RETURN_SYM						  \
 	     "movq "__percpu_arg([current_task])",%%rsi\n\t"		  \
