From: Thomas Gleixner <tglx@linutronix.de>
Date: Wed, 9 May 2018 23:01:01 +0200
Subject: x86/bugs, KVM: Extend speculation control for VIRT_SPEC_CTRL
Git-commit: ccbcd2674472a978b48c91c1fbfb66c0ff959f24 (partial)
Patch-mainline: v4.18 or v4.17-rc7 (next release)
References: bsc#1087082 CVE-2018-3639

AMD is proposing a VIRT_SPEC_CTRL MSR to handle the Speculative Store
Bypass Disable via MSR_AMD64_LS_CFG so that guests do not have to care
about the bit position of the SSBD bit and thus facilitate migration.
Also, the sibling coordination on Family 17H CPUs can only be done on
the host.

Extend x86_spec_ctrl_set_guest() and x86_spec_ctrl_restore_host() with an
extra argument for the VIRT_SPEC_CTRL MSR.

Hand in 0 from VMX and in SVM add a new virt_spec_ctrl member to the CPU
data structure which is going to be used in later patches for the actual
implementation.

[js] 4.4.144 brought part of the upstream commit in, omitting the kvm
     part. We do the kvm part here.

Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Reviewed-by: Borislav Petkov <bp@suse.de>
Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Acked-by: Borislav Petkov <bp@suse.de>
---
 arch/x86/kvm/svm.c |   11 +++++++++--
 arch/x86/kvm/vmx.c |    4 ++--
 2 files changed, 11 insertions(+), 4 deletions(-)

--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -177,6 +177,12 @@ struct vcpu_svm {
 	u64 next_rip;
 
 	u64 spec_ctrl;
+	/*
+	 * Contains guest-controlled bits of VIRT_SPEC_CTRL, which will be
+	 * translated into the appropriate L2_CFG bits on the host to
+	 * perform speculative control.
+	 */
+	u64 virt_spec_ctrl;
 
 	u64 host_user_msrs[NR_HOST_SAVE_USER_MSRS];
 	struct {
@@ -1542,6 +1548,7 @@ static void svm_vcpu_reset(struct kvm_vc
 	struct vcpu_svm *svm = to_svm(vcpu);
 	u32 dummy;
 	u32 eax = 1;
+	svm->virt_spec_ctrl = 0;
 
 	if (!init_event) {
 		svm->vcpu.arch.apic_base = APIC_DEFAULT_PHYS_BASE |
@@ -4832,7 +4839,7 @@ static void svm_vcpu_run(struct kvm_vcpu
 
 	local_irq_enable();
 
-	x86_spec_ctrl_set_guest(svm->spec_ctrl);
+	x86_spec_ctrl_set_guest(svm->spec_ctrl, svm->virt_spec_ctrl);
 
 	asm volatile (
 		"push %%" _ASM_BP "; \n\t"
@@ -4938,7 +4945,7 @@ static void svm_vcpu_run(struct kvm_vcpu
 	if (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
 		svm->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
 
-	x86_spec_ctrl_restore_host(svm->spec_ctrl);
+	x86_spec_ctrl_restore_host(svm->spec_ctrl, svm->virt_spec_ctrl);
 
 	reload_tss(vcpu);
 
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -8625,7 +8625,7 @@ static void __noclone vmx_vcpu_run(struc
 
 	debugctlmsr = get_debugctlmsr();
 
-	x86_spec_ctrl_set_guest(vmx->spec_ctrl);
+	x86_spec_ctrl_set_guest(vmx->spec_ctrl, 0);
 
 	vmx->__launched = vmx->loaded_vmcs->launched;
 	asm(
@@ -8745,7 +8745,7 @@ static void __noclone vmx_vcpu_run(struc
 	if (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
 		vmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
 
-	x86_spec_ctrl_restore_host(vmx->spec_ctrl);
+	x86_spec_ctrl_restore_host(vmx->spec_ctrl, 0);
 
 	/* Eliminate branch target predictions from guest mode */
 	vmexit_fill_RSB();
