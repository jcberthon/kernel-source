From: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date: Wed, 25 Apr 2018 22:04:19 -0400
Subject: x86/bugs, KVM: Support the combination of guest and host IBRS
Git-commit: 5cf687548705412da47c9cec342fd952d71ed3d5 (partial)
Patch-mainline: v4.17-rc7
References: bsc#1087082 CVE-2018-3639

A guest may modify the SPEC_CTRL MSR from the value used by the
kernel. Since the kernel doesn't use IBRS, this means a value of zero is
what is needed in the host.

But the 336996-Speculative-Execution-Side-Channel-Mitigations.pdf refers to
the other bits as reserved so the kernel should respect the boot time
SPEC_CTRL value and use that.

This allows to deal with future extensions to the SPEC_CTRL interface if
any at all.

Note: This uses wrmsrl() instead of native_wrmsl(). I does not make any
difference as paravirt will over-write the callq *0xfff.. with the wrmsrl
assembler code.

[js] 4.4.144 brought part of the upstream commit in, omitting the kvm
     part. We do the kvm part here.

Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
Reviewed-by: Borislav Petkov <bp@suse.de>
Reviewed-by: Ingo Molnar <mingo@kernel.org>
Acked-by: Borislav Petkov <bp@suse.de>
---
 arch/x86/kvm/svm.c |    9 ++-------
 arch/x86/kvm/vmx.c |    8 ++++----
 2 files changed, 6 insertions(+), 11 deletions(-)

--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -4832,8 +4832,7 @@ static void svm_vcpu_run(struct kvm_vcpu
 
 	local_irq_enable();
 
-	if (x86_ibrs_enabled() && (svm->spec_ctrl != SPEC_CTRL_IBRS))
-		wrmsrl(MSR_IA32_SPEC_CTRL, svm->spec_ctrl);
+	x86_spec_ctrl_set_guest(svm->spec_ctrl);
 
 	asm volatile (
 		"push %%" _ASM_BP "; \n\t"
@@ -4927,11 +4926,7 @@ static void svm_vcpu_run(struct kvm_vcpu
 #endif
 		);
 
-	if (x86_ibrs_enabled()) {
-		rdmsrl(MSR_IA32_SPEC_CTRL, svm->spec_ctrl);
-		if (svm->spec_ctrl != SPEC_CTRL_IBRS)
-			wrmsrl(MSR_IA32_SPEC_CTRL, SPEC_CTRL_IBRS);
-	}
+	x86_spec_ctrl_restore_host(svm->spec_ctrl);
 
 	/* Eliminate branch target predictions from guest mode */
 	vmexit_fill_RSB();
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -8623,12 +8623,10 @@ static void __noclone vmx_vcpu_run(struc
 
 	atomic_switch_perf_msrs(vmx);
 
-	if (boot_cpu_has(X86_FEATURE_SPEC_CTRL))
-		add_atomic_switch_msr(vmx, MSR_IA32_SPEC_CTRL,
-				      vmx->spec_ctrl, SPEC_CTRL_IBRS);
-
 	debugctlmsr = get_debugctlmsr();
 
+	x86_spec_ctrl_set_guest(vmx->spec_ctrl);
+
 	vmx->__launched = vmx->loaded_vmcs->launched;
 	asm(
 		/* Store host registers */
@@ -8747,7 +8745,9 @@ static void __noclone vmx_vcpu_run(struc
 
 	if (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))
 		vmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);
+
+	x86_spec_ctrl_restore_host(vmx->spec_ctrl);
 
 	/* Eliminate branch target predictions from guest mode */
 	vmexit_fill_RSB();
 
